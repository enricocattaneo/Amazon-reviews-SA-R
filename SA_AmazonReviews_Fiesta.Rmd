---
title: 'Sentiment Analysis of Amazon Product Reviews ("Fiesta: The Sun Also Rises" by E. Hemingway)'
author: "Enrico Cattaneo"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(rvest)
```

## Web Scraping 

Create a function that given the Amazon product ID retrives some valuable information (like product details and number of customer ratings)
```{r}
amazon_product_info <- function(id) {
  url <- paste0("https://www.amazon.co.uk/dp/", id)
  html <- read_html(url)
  
  # product details no rank nor n. reviews 
  product_details = html %>% 
    html_element("#detailBullets_feature_div") %>% 
    html_element("[class='a-unordered-list a-nostyle a-vertical a-spacing-none detail-bullet-list']") %>% 
    html_text2()
  
  # only the number of customers ratings
  number_of_ratings = html %>% 
    html_element("#acrCustomerReviewText") %>% 
    html_text2()

  # Return a tibble
  tibble(product_details, number_of_ratings) %>%
    return()
}
```

After choosing a product, we use the previous function to obtain information about it.
```{r}
id_prod = "0099908506" # id product
prod_info = amazon_product_info(id_prod)
prod_info
```
#### Scrape Product Reviews

Create a function to obtain the product reviews (title, text, review stars), considering both UK reviews and not from UK ones.
```{r}
amazon_reviews <- function(id, page) {
  url <- paste0("https://www.amazon.co.uk/product-reviews/", # url using id and page variables 
                id, "/?pageNumber=", page)
  html <- read_html(url)
  
  # Review title (UK and not-UK)
  title = html %>%
    html_elements("[class='a-size-base a-link-normal review-title a-color-base review-title-content a-text-bold']") %>%
    html_text2()
  
  title = title %>%
    c(html %>%
        html_elements("[class='a-size-base review-title a-color-base review-title-content a-text-bold']") %>%
        html_text2())
  
  # Review text (the same for UK and not-UK)
  text = html %>%
    html_elements("[class='a-size-base review-text review-text-content']") %>%
    html_text2()
  
  # Review stars (UK and not-UK)
  star = html %>%
    html_elements("[data-hook='review-star-rating']") %>%
    html_text2()
  
  star = star %>%
    c(html %>%
        html_elements("[data-hook='cmps-review-star-rating']") %>%
        html_text2())
  
  # Return a tibble
  tibble(title, text, star, page = page) %>%
    return()
}
```


With map_df function from the purrr package we can iterate the task over multiple pages to create a dataframe.

```{r}
library(purrr)
page = 1:30
prod_rev = map_df(page, ~amazon_reviews(id_prod, page = .))

prod_rev$doc_id = 1:nrow(prod_rev)  # we also add a doc_id and we save the results 
head(prod_rev)
```
 
## Data Cleaning and Pre-Processing
 
#### Language Detection 
Consider only English written reviews

```{r}
library(cld2) # if the language cannot be determined it returns NA.

prod_rev$title_lang = detect_language(prod_rev$title)
prod_rev$text_lang = detect_language(prod_rev$text)
table(prod_rev$text_lang, prod_rev$title_lang, useNA = "always") # compare the results using table  

prod_rev = prod_rev %>% 
  filter(text_lang == "en") # select only reviews in english
prod_rev
```



#### SCORE 
Extract a numeric score from the stars string
```{r}
# Convert stars from string to numeric
prod_rev = prod_rev %>% 
  mutate(score = as.numeric(substring(star, 1, 1)))
summary(prod_rev$score)
# Compute distribution of stars + visualization
prod_rev %>% 
  count(score) %>% 
  mutate(p = round(n/sum(n), 2))
prod_rev %>%
  ggplot(aes(x = score)) + geom_bar(aes(y = (..count..)), fill = "steelblue") + 
  labs(title = "Amazon reviews' stars", x = "Stars", y = "Number of comments") + 
  theme_bw() + 
  theme(plot.title = element_text(color = "steelblue", size = 12, face = "bold"), 
        plot.subtitle = element_text(color = "steelblue2"))
```

It appears that positive reviews prevail. From the 5 class score, we can tranform it to a binary classification: if the reviews has 4 or 5 starts it is positive, otherwise it is negative.

```{r}
# Binary variable creation
prod_rev = prod_rev %>% 
  mutate(star_sent = ifelse(star>=4, "positive", "negative"))
# Binary variable's distribution
prod_rev %>% 
  count(star_sent) %>% 
  mutate(p = round(n/sum(n), 2))
```

# We can also compare some features differences between positive and negative reviews (like length of text).

```{r}
prod_rev$nchar = str_length(prod_rev$text)
ggplot(prod_rev, aes(x = star_sent, y = nchar, fill = star_sent)) + 
  geom_boxplot() +
  theme_bw() +
  scale_fill_manual(values = c("steelblue", "skyblue"))
```


#### Text Cleaning

In order to conduct a better analysis we need to clean the text data making it easier to work with. Stop-words (customized in our case), upper-case letters, punctuaction and digits are dropped. 

We create are own custom-stopwords because we have a problem with the ' symbol (it isn't detect when it appear as ’) with the filtering option we would delete even some non-stop-words so we create custom_stopwords.

```{r}
library(tidytext) 
# Create our custom stop-words
custom_stopwords = bind_rows(
  tibble(word = c(
    "t’s","i’m","you’re","he’s","she’s","it’s","we’re","they’re","i’ve","you’ve","we’ve","they’ve","i’d","you’d","he’d","she’d",
    "we’d","they’d","i’ll","you’ll","he’ll", "she’ll","we’ll","they’ll","isn’t","aren’t","wasn’t","weren’t","hasn’t",
    "haven’t","hadn’t","doesn’t","don’t","didn’t","won’t","wouldn’t","shan’t","shouldn’t","can’t","cannot","couldn’t","mustn’t",
    "let’s","that’s","who’s", "what’s","here’s","there’s","when’s","where’s","why’s","how’s","a’s","ain’t", "c’s","c’mon"),
    lexicon = "custom"), stop_words) 
# Filter out unwanted words and symbols
tidy_text = prod_rev %>% 
  unnest_tokens(word, text) %>% 
  anti_join(custom_stopwords) %>% 
  filter(!str_detect(word, "^([[:digit:]]+)$")) %>%  # filter for numbers (~130 words)
  filter(!str_detect(word, "^([[:alnum:]]+)[.,]([[:alnum:]]+)")) # filter for numbers with decimal (few words) 
                                                                 # + word.word(mistakes in punctuation)(~300 words)
# Look at some frequent terms
freq.df = tidy_text %>%
  count(word, sort = T)
head(freq.df, 20)
```

#### Word Normalization
For word normalization we could use either stemming or lemmatization. The goal of both methods is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For our analysis we use Stemming, which is the process of reducing the word to its root eliminating the suffix.

```{r}
# STEMMING
library(SnowballC)
tidy_stem = tidy_text %>%
  mutate(word = wordStem(word))

# LEMMATIZATION
library(udpipe)
tidy_lemma <- udpipe(prod_rev, "english-gum")
tidy_lemma = tidy_lemma %>%
  mutate(stem = wordStem(token)) %>%
  tibble()

tidy_lemma # table and the differences between token (word) lemmas and stems:
tidy_lemma %>%
  select(token, lemma, stem)
```


## Dictionary-based Sentiment Analysis 

### Tidy Approach

We first consider the tidy approach, where we consider words as tokens. With this SA approach, we will use three lexicons: BING (gives words a positive or negative sentiment), AFINN (rates words with a value from -5 to +5), and NRC (labels words with six possible sentiments or emotions).The procedure for each of these lexicons is similar, but the results are dependent on the lexicon itself. With every specific lexicon, we are able to give a sentiment or value to (almost) each word, and then we compute the value of each review as an aggregation of the contained words’ values/sentiment. We later plot our results using histograms.


##### **Bing Lexicon**

```{r}
bing = get_sentiments("bing")
# Get sentiment score 
prod_rev_bing = tidy_text %>%
  select(doc_id, word) %>%
  inner_join(bing) %>%
  count(doc_id, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  mutate(bing = positive - negative)

prod_rev = prod_rev %>%
  left_join(prod_rev_bing %>%
              select(doc_id, bing))

hist(prod_rev$bing, col = "red", main = "Sentiment distribution - tidy- bing lexicon") 
summary(prod_rev$bing)

# Analyze different words' contribution to the sentiment.
bing_word_counts <- tidy_text %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 5, with_ties = F) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) + 
  geom_col(show.legend = F) + 
  facet_wrap(~sentiment, scales = "free_y") + 
  labs(x = "Contribution to sentiment", y = NULL) +
  theme_bw() + scale_fill_manual(values = c("steelblue","skyblue"))
```
  
We can also plot a word-cloud. The color represent the sentiment associated to a particular word, while the size of each word depends on the its frequency.
                                                                                                         
```{r}
library(wordcloud)
library(wordcloud2)
library(reshape2)

tidy_text %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("orangered", "darkgreen"), max.words = 100)
```

##### **AFINN LEXICON**
We conduct the same analysis as before, using the AFINN lexicon (need for few arrangments).

```{r}
afinn = get_sentiments("afinn")
# Get sentiment score 
prod_rev_afinn = tidy_text %>%
  select(doc_id, word) %>%
  inner_join(afinn) %>%
  group_by(doc_id) %>% 
  summarise(afinn = sum(value))

prod_rev = prod_rev %>%
  left_join(prod_rev_afinn %>%
              select(doc_id, afinn))

hist(prod_rev$afinn, col = "blue", main = "Sentiment distribution - tidy - afinn lexicon") 
summary(prod_rev$afinn)

# Let's see the contribution of words to the sentiment.
afinn_word_counts <- tidy_text %>%
  inner_join(get_sentiments("afinn")) %>%
  count(word, value, sort = TRUE) %>%
  ungroup()

afinn_word_counts %>%
  group_by(value) %>%
  slice_max(n, n = 5, with_ties = F) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = value)) + geom_col(show.legend = F) + 
  facet_wrap(~value, scales = "free_y") + labs(x = "Contribution to sentiment", 
                                                   y = NULL) 
```

##### **NRC LEXICON**
We conduct the same analysis as before, using the NRC lexicon (need for few arrangments).

```{r}
nrc = get_sentiments("nrc")
# Get sentiment score 
prod_rev_nrc = tidy_text %>%
  select(doc_id, word) %>%
  inner_join(nrc) %>%
  count(doc_id, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  mutate(nrc = positive - negative)

prod_rev = prod_rev %>%
  left_join(prod_rev_nrc %>%
              select(doc_id, nrc))

hist(prod_rev$nrc, col = "yellow", main = "Sentiment distribution - tidy - nrc lexicon") 
summary(prod_rev$nrc)

# Let's see the contribution of words to the sentiment.
nrc_word_counts <- tidy_text %>%
  inner_join(get_sentiments("nrc")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

nrc_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 5, with_ties = F) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) + geom_col(show.legend = FALSE) + 
  facet_wrap(~sentiment, scales = "free_y") + labs(x = "Contribution to sentiment", 
                                                   y = NULL) + theme_bw() 
```

##### Comparison Between Different Lexicons

Lexicons histogram - comparing sentiment distribution using different lexicons

```{r}
prod_rev %>% 
  ggplot() + 
  geom_histogram(aes(x = bing, fill = "b"), bins = 40, alpha = 0.5) +
  geom_histogram(aes(x = afinn, fill = "a"), bins = 40, alpha = 0.5) +
  geom_histogram(aes(x = nrc, fill = "n"), bins = 40 , alpha = 0.5) +
  scale_fill_manual(name ="lexicon", values = c("b" = "red", "a" = "blue", "n" = "yellow"),
                    labels=c("b" = "bing", "a" = "afinn", "n" = "nrc")) +
  labs(title= "Sentiment Distribution using all 3 lexicons", y = "Frequency", x = "Sentiment")
```
Word count sentiments - compare most common positive/negative words (considering different lexicons)

```{r}
#BING 
bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 5, with_ties = F) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) + 
  geom_col(show.legend = F) + 
  facet_wrap(~sentiment, scales = "free_y") + 
  labs(x = "Contribution to sentiment - BING", y = NULL) +
  theme_bw() + scale_fill_manual(values = c("red4","red"))

# AFINN
afinn_word_counts %>%
  group_by(sentiment = ifelse(value>0, "positive", "negative")) %>%
  slice_max(n, n = 5, with_ties = F) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) + 
  geom_col(show.legend = F) + 
  facet_wrap(~sentiment, scales = "free_y") + 
  labs(x = "Contribution to sentiment - AFINN", y = NULL) +
  theme_bw() + scale_fill_manual(values = c("steelblue","skyblue"))

# NRC
nrc_word_counts %>%
  filter(sentiment %in% c("positive", "negative")) %>% 
  group_by(sentiment) %>%
  slice_max(n, n = 5, with_ties = F) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) + geom_col(show.legend = FALSE) + 
  facet_wrap(~sentiment, scales = "free_y") + labs(x = "Contribution to sentiment - NRC", y = NULL) + theme_bw() +
  scale_fill_manual(values = c("goldenrod3","gold")) 
```


### Udpipe Approach

With this approach, we also consider polarity negators and polarity amplifiers (we will consider the previous 2 words, not following words). The performance increases when we consider them both. However, also this approach is not free from possible problems, there is some situation in which the approach under-perform the previous one. We can use lemmas or words in the analysis and we can use one from the three lexicons (will not drop stop-words). 

##### **Bing Lexicon with Lemmas**

```{r}
library(udpipe)
data_udpipe <- udpipe(prod_rev, "english-gum")

bing_dict = get_sentiments("bing") %>%
  mutate(sentiment = ifelse(sentiment == "negative", -1, 1)) %>%
  rename(term = "word", polarity = "sentiment")

scores_b <- txt_sentiment(x = data_udpipe, 
                        term = "lemma", #in this case we use lemmas instead of words 
                        polarity_terms = bing_dict, #we also not dropping stop-words
                        polarity_negators = "not", #there 'll be some difference 
                        polarity_amplifiers = "very", 
                        n_before = 2, 
                        n_after = 0,
                        constrain = F)
prod_rev$udpipe_bing_l = scores_b$overall$sentiment_polarity

summary(prod_rev$udpipe_bing_l)
summary(prod_rev$bing)
# Compare distributions between the tidy approach using Bing and the udpipe approach using the same lexicon (and lemmas)
par(mfrow = c(1, 2))
hist(scale(prod_rev$bing), col = "lightblue", main = "Sentiment distribution-bing")
hist(scale(prod_rev$udpipe_bing_l), col = "lightblue", main = "udpipe (bing dict-lemmas)")
```


##### **Bing Lexicon with Words**
```{r}
scores_c <- txt_sentiment(x = data_udpipe, 
                        term = "token", #in this case we use lemmas instead of words 
                        polarity_terms = bing_dict, #we also not dropping stop-words
                        polarity_negators = c("not"), #there'll be some difference 
                        polarity_amplifiers = c("very"), 
                        n_before = 2,
                        n_after = 0,
                        constrain = F)
prod_rev$udpipe_bing_w = scores_c$overall$sentiment_polarity
summary(prod_rev$udpipe_bing_w)
# Compare distributions between the tidy approach using Bing, the udpipe approach with lemmas, and the udpipe approach with words
par(mfrow = c(1, 3))
hist(scale(prod_rev$bing), col = "lightblue", main = "Sentiment distribution - bing") # tidy approach 
hist(scale(prod_rev$udpipe_bing_w), col = "lightblue", main = "udpipe (bing dict) - words") # udpipe approach with words 
hist(scale(prod_rev$udpipe_bing_l), col = "lightblue", main = "udpipe (bing dict-lemmas)") # udpipe with lemmas 
```

We can repeat all this process for all the other lexicons.

##### **Afinn Lexicon with Lemmas**

```{r}
afinn_dict = get_sentiments("afinn") %>%
  rename(term = "word", polarity = "value")

data_udpipe <- udpipe(prod_rev, "english-gum")

scores_a <- txt_sentiment(x = data_udpipe, 
                        term = "lemma", #in this case we use lemmas instead of words 
                        polarity_terms = afinn_dict, #we also not dropping stop-words
                        polarity_negators = c("not"), #there'll be some difference 
                        polarity_amplifiers = c("very"), 
                        n_before = 2,
                        n_after = 0,
                        constrain = F)
prod_rev$udpipe_afinn_l = scores_a$overall$sentiment_polarity
hist(prod_rev$udpipe_afinn_l, col = "lightblue", main = "Sentiment distribution - udpipe (afinn dict - lemmas)")
summary(prod_rev$udpipe_afinn_l)
summary(prod_rev$afinn)
# Compare distributions between the tidy approach using Afinn and the udpipe approach using the same lexicon (and lemmas)
par(mfrow = c(1, 2))
hist(scale(prod_rev$afinn), col = "lightblue", main = "Sentiment distribution - afinn")
hist(scale(prod_rev$udpipe_afinn_l), col = "lightblue", main = "Sentiment distribution - udpipe (afinn dict - lemmas)")
```



##### **Afinn Lexicon with Words**
```{r}
scores_a <- txt_sentiment(x = data_udpipe, 
                        term = "token", #in this case we use lemmas instead of words 
                        polarity_terms = afinn_dict, #we also not dropping stop-words
                        polarity_negators = c("not"), #there'll be some difference 
                        polarity_amplifiers = c("very"), 
                        n_before = 2,
                        n_after = 0,
                        constrain = F)
prod_rev$udpipe_afinn_w = scores_a$overall$sentiment_polarity
hist(prod_rev$udpipe_afinn_w, col = "lightblue", main = "Sentiment distribution - udpipe (afinn dict - words)")
summary(prod_rev$udpipe_afinn_w)

# Compare distributions between the tidy approach using Afinn, the udpipe approach with lemmas, and the udpipe approach with words
par(mfrow = c(1, 3))
hist(scale(prod_rev$afinn), col = "lightblue", main = "Sentiment distribution - afinn")
hist(scale(prod_rev$udpipe_afinn_l), col = "lightblue", main = "Sentiment distribution - udpipe (afinn dict - lemmas)")
hist(scale(prod_rev$udpipe_afinn_w), col = "lightblue", main = "Sentiment distribution - udpipe (afinn dict - words)")
```

##### **NRC Lexicon with Lemmas**

```{r}
nrc_dict = get_sentiments("nrc") %>%
  mutate(sentiment = ifelse(sentiment == "negative", -1, 1)) %>%
  rename(term = "word", polarity = "sentiment")

data_udpipe <- udpipe(prod_rev, "english-gum")

scores_n <- txt_sentiment(x = data_udpipe, 
                        term = "lemma", #in this case we use lemmas instead of words 
                        polarity_terms = nrc_dict, #we also not dropping stop-words
                        polarity_negators = c("not"), #there'll be some difference 
                        polarity_amplifiers = c("very"), 
                        n_before = 2,
                        n_after = 0,
                        constrain = F)
prod_rev$udpipe_nrc_l = scores_n$overall$sentiment_polarity
summary(prod_rev$udpipe_nrc_l)
summary(prod_rev$nrc)
# Compare distributions between the tidy approach using NRC and the udpipe approach using the same lexicon (and lemmas)
par(mfrow = c(1, 2))
hist(scale(prod_rev$nrc), col = "lightblue", main = "Sentiment distribution - nrc")
hist(scale(prod_rev$udpipe_nrc_l), col = "lightblue", main = "Sentiment distribution - udpipe (nrc dict - lemmas)")
```

##### **NRC Lexicon with Lemmas**
```{r}
scores_n <- txt_sentiment(x = data_udpipe, 
                        term = "token", #in this case we use lemmas instead of words 
                        polarity_terms = nrc_dict, #we also not dropping stop-words
                        polarity_negators = c("not"), #there'll be some difference 
                        polarity_amplifiers = c("very"), 
                        n_before = 2,
                        n_after = 0,
                        constrain = F)
prod_rev$udpipe_nrc_w = scores_n$overall$sentiment_polarity
hist(prod_rev$udpipe_nrc_w, col = "lightblue", main = "Sentiment distribution - udpipe (nrc dict - words)")
summary(prod_rev$udpipe_nrc_w)

# Compare distributions between the tidy approach using Afinn, the udpipe approach with lemmas, and the udpipe approach with words
par(mfrow = c(1, 3))
hist(scale(prod_rev$nrc), col = "lightblue", main = "Sentiment distribution - nrc")
hist(scale(prod_rev$udpipe_nrc_l), col = "lightblue", main = "Sentiment distribution - udpipe (nrc dict - lemmas)")
hist(scale(prod_rev$udpipe_nrc_w), col = "lightblue", main = "Sentiment distribution - udpipe (nrc dict - words)")
```

### Does the Sentiment reflect reviews' stars?

##### Using udpipe approach considering lemmas only, for different lexicons.

```{r}
# BING
prod_rev %>%
  select(doc_id, star_sent, udpipe_bing_l, bing) %>%
  mutate(star_sent = ifelse(star_sent == "positive", 1, -1), 
         udpipe_bing_l = ifelse(udpipe_bing_l > 0, 1, ifelse(udpipe_bing_l < 0, -1, 0)), 
         bing = ifelse(bing > 0, 1, ifelse(bing < 0, -1, 0)), bing = replace_na(bing, 0)
         ) %>%
  pivot_longer(cols = c("star_sent", "udpipe_bing_l", "bing")) %>%
  ggplot(aes(doc_id, value, fill = name)) + geom_col(show.legend = FALSE) + 
  facet_wrap(~name, ncol = 1, scales = "free_y", strip.position = "right") + 
  theme_bw() + scale_fill_manual(values = c("deepskyblue2", "steelblue", "deepskyblue2")) + ggtitle('Compare: Tidy SA, Udpipe SA with lemmas, and Reviews stars. (Using BING)')

# AFINN
prod_rev %>%
  select(doc_id, star_sent, udpipe_afinn_l, afinn) %>%
  mutate(star_sent = ifelse(star_sent == "positive", 1, -1), 
         udpipe_afinn_l = ifelse(udpipe_afinn_l > 0, 1, ifelse(udpipe_afinn_l < 0, -1, 0)), 
         afinn = ifelse(afinn > 0, 1, ifelse(afinn < 0, -1, 0)), afinn = replace_na(afinn, 0)) %>%
  pivot_longer(cols = c("star_sent", "udpipe_afinn_l", "afinn")) %>%
  ggplot(aes(doc_id, value, fill = name)) + geom_col(show.legend = FALSE) + 
  facet_wrap(~name, ncol = 1, scales = "free_y", strip.position = "right") + 
  theme_bw() + scale_fill_manual(values = c("deepskyblue2", "steelblue", "deepskyblue2")) + ggtitle('Compare: Tidy SA, Udpipe SA with lemmas, and Reviews stars. (Using AFINN)')

#NRC
prod_rev %>%
  select(doc_id, star_sent, udpipe_nrc_l, nrc) %>%
  mutate(star_sent = ifelse(star_sent == "positive", 1, -1), 
         udpipe_nrc_l = ifelse(udpipe_nrc_l > 0, 1, ifelse(udpipe_nrc_l < 0, -1, 0)), 
         nrc = ifelse(nrc > 0, 1, ifelse(nrc < 0, -1, 0)), nrc = replace_na(nrc, 0)) %>%
  pivot_longer(cols = c("star_sent", "udpipe_nrc_l", "nrc")) %>%
  ggplot(aes(doc_id, value, fill = name)) + geom_col(show.legend = FALSE) + 
  facet_wrap(~name, ncol = 1, scales = "free_y", strip.position = "right") + 
  theme_bw() + scale_fill_manual(values = c("deepskyblue2", "steelblue", "deepskyblue2"))+ ggtitle('Compare: Tidy SA, Udpipe SA with lemmas, and Reviews stars. (Using NRC)')
```

In all these cases there are some differences. We can also compare the sentiments with the star score (pretending that it is the true one). 
Notice how these results strongly depends on the pre-pocessing phase (for the tidy approach we eliminated stropwords, for the udpipe one we considered lemmas instead of words and we didn't remove stopwords).

### Visualizations

##### Unigram

We start by looking at the most frequent stems in the whole corpus (all the documents).

```{r}
tidy_stem %>%
  count(word) %>%
  slice_max(n, n = 10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = word)) + geom_bar(stat = "identity", fill = "skyblue") + 
  xlab(NULL) + labs(title = "Most common stems in reviews", y = "Stems count") +
  theme(legend.position = "none", plot.title = element_text(color = "steelblue", size = 12, face = "bold")) +
  coord_flip() + theme_bw()
```

Then, we can compare the stems used by people who wrote positive and negative reviews respectively.

```{r}
tidy_stem %>%
  group_by(star_sent) %>%
  count(word) %>%
  group_by(star_sent) %>%
  slice_max(n, n = 10, with_ties = F) %>%
  mutate(star_sent = as.factor(star_sent), word = reorder_within(word,n, star_sent)) %>%
  ggplot(aes(word, n, fill = star_sent)) +
  geom_col(show.legend = FALSE) + 
  facet_wrap(~star_sent, scales = "free_y") + 
  coord_flip() +
  labs(title = "Most common stems in positive/negative reviews",y = NULL, x = "N") +
  scale_x_reordered() + theme(legend.position = "none",plot.title = element_text(color = "orangered", "dodgerblue")) +
  scale_fill_manual(values = c("orangered", "dodgerblue")) + theme_bw()
```

In order to show which stems are important but specific to each cateogory we can provide different visualization/scores. 

We use a geom_jitter to compare the frequency of stems in positive and negative comments. The stems which lie near to the red line are used with about the same frequency in the two categories.

```{r}
tidy_stem %>%
  group_by(star_sent) %>%
  count(word, sort = T) %>%
  mutate(prop = n/sum(n)) %>%
  select(star_sent, word, prop) %>%
  pivot_wider(names_from = star_sent, values_from = prop) %>%
  arrange(positive, negative) %>%
  ggplot(aes(positive, negative)) + 
  geom_jitter(alpha = 0.5,size = 2.5, width = 0.25, height = 0.25, colour = "steelblue") +
  geom_text(aes(label = word), check_overlap = T, vjust = 0) +
  scale_x_log10() +
  scale_y_log10() +
  geom_abline(color = "red") + theme_bw()
```

# We can compare the log-odds ratio to understand which words are more or less likely to come from each cateogry of reviews (positive or negative).

```{r}
word_ratios <- tidy_stem %>%
  count(word, star_sent) %>%
  group_by(word) %>%
  filter(sum(n) >= 10) %>%
  ungroup() %>%
  pivot_wider(names_from = star_sent, values_from = n, values_fill = 0) %>%
  mutate_if(is.numeric, list(~(. + 1)/(sum(.) + 1))) %>%
  mutate(logratio = log(positive/negative)) %>%
  arrange(desc(logratio))

word_ratios %>%
  group_by(logratio < 0) %>%
  slice_max(abs(logratio), n = 15) %>%
  ungroup() %>%
  mutate(word = reorder(word, logratio)) %>%
  ggplot(aes(word, logratio, fill = logratio < 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip() + ylab("log odds ratio (Positive/Negative)") +
  scale_fill_manual(name = "", labels = c("Positive", "Negative"),values = c("dodgerblue", "orangered")) + theme_bw()
```

We can also can plot some wordclouds. 
```{r}
tidy_stem %>%
  count(word) %>%
  with(wordcloud(scale = c(5, 0.7), word, n, max.words = 100,
                 min.freq = 2, random.order = F, rot.per = 0.15, colors = brewer.pal(8, "Paired")))
# we use the words instead of the stems and the wordcloud2 package.
frame = tidy_text %>%
  count(word, sort = T)
frame = data.frame(word = frame$word, freq = frame$n)
wordcloud2(frame, color = "skyblue")
```

##### Bigrams

We can show some of the previous plots also for bigrams. Let's consider a new type of visualization. More precisely, if you are interested in the relationship between words, it is useful to consider a network (with also the "direction" of the link).
```{r}
library(ggraph)
library(igraph)

tidy_big_stem <- prod_rev %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  mutate(word1 = wordStem(word1)) %>%
  mutate(word2 = wordStem(word2))

bigram_counts = tidy_big_stem %>%
  count(word1, word2, sort = TRUE)
bigram_graph <- bigram_counts %>%
  filter(n >= 2) %>% 
  graph_from_data_frame()


set.seed(9265)
a <- grid::arrow(type = "closed", length = unit(0.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a, end_cap = circle(1, "inches")) +
  geom_node_point(color = "skyblue", size = 3) + 
  geom_node_text(aes(label =name), vjust = 1, hjust = 1) + theme_void()
```

##### Bigrams Using POS

```{r}
cooc <- cooccurrence(tidy_lemma$lemma, relevant = tidy_lemma$upos %in% c("NOUN", "ADJ"), skipgram = 1)
head(cooc)

wordnetwork <- head(cooc, 15)
wordnetwork <- graph_from_data_frame(wordnetwork) 

ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc,edge_alpha = cooc), edge_colour = "skyblue") +
  geom_node_text(aes(label = name),col = "darkblue", size = 4) +
  theme_void() + labs(title = "Words following one another",subtitle = "Nouns & Adjective")
```

##### Co-Occurency Using POS

```{r}
cooc <- cooccurrence(x = subset(tidy_lemma, upos %in% c("NOUN", "ADJ")), term = "lemma", group = c("doc_id"))
head(cooc)

wordnetwork <- head(cooc, 30)
wordnetwork <- graph_from_data_frame(wordnetwork) 

ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc,edge_alpha = cooc), edge_colour = "skyblue") +
  geom_node_text(aes(label = name),col = "darkblue", size = 4) +
  theme(legend.position = "none") + theme_void() + 
  labs(title = "Cooccurrences within documents", subtitle = "Nouns & Adjective")
```

##### Dependency Parsing

```{r}
library(textplot)
textplot_dependencyparser(tidy_lemma %>%filter(doc_id == "1" & sentence_id == "1"))
```
